from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def unet_model(input_size=(512, 512, 4), l2_reg=0.001):
    inputs = Input(input_size)
    
    # Encoder
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(inputs)
    conv1 = BatchNormalization()(conv1)
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(conv1)
    conv1 = BatchNormalization()(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)


    # Repeat similar blocks for deeper layers...

    # Bottleneck
    convn = Conv2D(1024, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(pooln)
    convn = BatchNormalization()(convn)
    convn = Dropout(0.5)(convn)
    convn = Conv2D(1024, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(convn)
    convn = BatchNormalization()(convn)

    # Decoder
    upn = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(convn)
    upn = concatenate([upn, conv5])
    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(upn)
    conv5 = BatchNormalization()(conv5)

    # Repeat similar blocks for upper layers...

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv1)
    model = Model(inputs=[inputs], outputs=[outputs])
    
    return model
